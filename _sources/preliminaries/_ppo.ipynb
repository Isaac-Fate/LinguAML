{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "from gym import Env\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Beta\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "max_episode_steps = 100\n",
    "replay_buffer_capacity = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\", max_episode_steps=max_episode_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateActionSpec:\n",
    "    \n",
    "    def __init__(self, env: Env) -> None:\n",
    "        \n",
    "        self._env = env\n",
    "    \n",
    "    @property\n",
    "    def state_dim(self) -> Optional[int]:\n",
    "        \n",
    "        try:\n",
    "            return int(self._env.observation_space.shape[0])\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def action_dim(self) -> Optional[int]:\n",
    "        \n",
    "        try:\n",
    "            return int(self._env.action_space.shape[0])\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    @property\n",
    "    def n_actions(self) -> Optional[int]:\n",
    "        \n",
    "        try:\n",
    "            return self._env.action_space.n\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def action_min(self) -> Optional[np.ndarray]:\n",
    "        \n",
    "        try:\n",
    "            return env.action_space.__dict__[\"low\"]\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @property\n",
    "    def action_max(self) -> Optional[np.ndarray]:\n",
    "        \n",
    "        try:\n",
    "            return env.action_space.__dict__[\"high\"]\n",
    "        except:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_action_spec = StateActionSpec(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\n",
    "    \"Transition\",\n",
    "    (\n",
    "        \"state\",\n",
    "        \"action\",\n",
    "        \"next_state\",\n",
    "        \"reward\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        \n",
    "        self._capacity = capacity\n",
    "        self._transitions = deque([], maxlen=self._capacity)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._transitions)\n",
    "    \n",
    "    @property\n",
    "    def capacity(self) -> int:\n",
    "        \"\"\"Maximum number of transitions in memory.\n",
    "        \"\"\"\n",
    "        return self._capacity\n",
    "    \n",
    "    def add(self, *args) -> None:\n",
    "        \n",
    "        if len(args) == 1:\n",
    "            transition = args[0]\n",
    "            assert isinstance(transition, Transition)\n",
    "            \n",
    "        elif len(args) == len(Transition._fields):\n",
    "            transition = Transition(*args)\n",
    "            \n",
    "        else:\n",
    "            raise ValueError\n",
    "        \n",
    "        self._transitions.append(transition)\n",
    "        \n",
    "    def sample(self, batch_size: int) -> list[Transition]:\n",
    "        \n",
    "        return random.sample(\n",
    "            population=self._transitions, \n",
    "            k=batch_size\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(replay_buffer_capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            state_action_spec: StateActionSpec\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self._state_action_spec = state_action_spec\n",
    "        self.fc = nn.Linear(\n",
    "            self._state_action_spec.state_dim, \n",
    "            64\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=64, \n",
    "            hidden_size=128,\n",
    "            batch_first=True\n",
    "        )\n",
    "    \n",
    "    @property\n",
    "    def state_action_spec(self) -> StateActionSpec:\n",
    "        return self._state_action_spec\n",
    "    \n",
    "    @property\n",
    "    def n_features(self) -> int:\n",
    "        return self.lstm.hidden_size\n",
    "        \n",
    "    def forward(self, states: Tensor) -> Tensor:\n",
    "        \"\"\"Calculate the features of a sequence of states.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        states : Tensor\n",
    "            Batches of obserbed states.\n",
    "            Shape: (N, state_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Batches of sequences of output tensors from LSTM layer.\n",
    "            Shape: (N, 1, H)\n",
    "        \"\"\"\n",
    "        \n",
    "        n_batches = states.shape[0]\n",
    "        \n",
    "        x = self.fc(states)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(n_batches, 1, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.squeeze(dim=1)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            feature_network: FeatureNetwork\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_network = feature_network\n",
    "        \n",
    "        self.fc = nn.Linear(\n",
    "            self.feature_network.n_features, \n",
    "            2 * self.feature_network.state_action_spec.action_dim\n",
    "        )\n",
    "        \n",
    "    def forward(self, states: Tensor) -> Tensor:\n",
    "        \"\"\"Determine the actions (as in parameters of normal distributions) \n",
    "        to take based on a sequence of states.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        states : Tensor\n",
    "            Batches of obserbed states.\n",
    "            Shape: (N, state_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Batches of actions to take.\n",
    "            Shape: (N, 2 * action_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = states.shape[0]\n",
    "        x = self.feature_network(states)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(batch_size, -1, 2)\n",
    "\n",
    "        x = F.softmax(x, dim=-1) * 10\n",
    "        \n",
    "        alpha = x[..., 0]\n",
    "        beta = x[..., 1]\n",
    "        sample = Beta(alpha, beta).sample()\n",
    "        sample.requires_grad_()\n",
    "        action_min = torch.tensor(self.feature_network.state_action_spec.action_min)\n",
    "        action_max = torch.tensor(self.feature_network.state_action_spec.action_max)\n",
    "        action = action_min + sample * (action_max - action_min)\n",
    "        \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            feature_network: FeatureNetwork\n",
    "        ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.feature_network = feature_network\n",
    "        \n",
    "        self.fc = nn.Linear(self.feature_network.n_features, 1)\n",
    "        \n",
    "    def forward(self, states: Tensor) -> Tensor:\n",
    "        \"\"\"Calculate state values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        states : Tensor\n",
    "            Batches of obserbed states.\n",
    "            Shape: (N, state_dim)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Batches of state values.\n",
    "            Shape: (N,)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = self.feature_network(states)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_network = FeatureNetwork(state_action_spec)\n",
    "policy_network = PolicyNetwork(feature_network)\n",
    "target_network = TargetNetwork(feature_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7457,  1.0544,  0.3091],\n",
       "        [ 0.6257,  0.4877, -1.6655],\n",
       "        [-0.0422, -0.5164, -0.7973],\n",
       "        [-0.4540, -0.2207,  1.4826]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4, 3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1885],\n",
       "        [ 0.1156],\n",
       "        [-0.0206],\n",
       "        [ 0.1475]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0417, -0.0764, -0.0544, -0.0449], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_one_episode(env: Env, replay_buffer: ReplayBuffer):\n",
    "    \n",
    "    state, _ = env.reset()\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        states = torch.tensor(state).unsqueeze(dim=0)\n",
    "        actions: Tensor = policy_network(states)\n",
    "        action = actions.detach().squeeze(dim=0).numpy()\n",
    "        \n",
    "        print(action)\n",
    "        \n",
    "        # Interact with the environment\n",
    "        next_state, reward, is_terminated, is_truncated, _ = env.step(action)\n",
    "        is_done = is_terminated or is_truncated\n",
    "        \n",
    "        # Add this transition to memory\n",
    "        replay_buffer.add(state, action, next_state, reward)\n",
    "        \n",
    "        state = next_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_one_episode(env, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step((1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(state).unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_network(torch.tensor(state).unsqueeze(dim=0)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linguaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
