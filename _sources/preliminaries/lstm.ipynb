{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14998ee10>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Self, Optional\n",
    "from pprint import pprint\n",
    "from io import StringIO\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import sigmoid, tanh\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model parameters: odict_keys(['weight_ih', 'weight_hh', 'bias_ih', 'bias_hh'])\n"
     ]
    }
   ],
   "source": [
    "# Input size of x, i.e.,\n",
    "# number of features of x\n",
    "d = 3\n",
    "\n",
    "# Hidden size, i.e.,\n",
    "# size of the hidden state vector h\n",
    "# In fact, all vectors involved in an LSTM cell \n",
    "# other than x share the same size\n",
    "k = 2\n",
    "\n",
    "lstm_cell = nn.LSTMCell(\n",
    "    input_size=d,\n",
    "    hidden_size=k,\n",
    ")\n",
    "\n",
    "print(f\"model parameters: {lstm_cell.state_dict().keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:\n",
      "W_ih: torch.Size([8, 3])\tbias_ih: torch.Size([8])\n",
      "W_hh: torch.Size([8, 2])\tbias_hh: torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "W_ih = lstm_cell.state_dict()[\"weight_ih\"]\n",
    "bias_ih = lstm_cell.state_dict()[\"bias_ih\"]\n",
    "W_hh = lstm_cell.state_dict()[\"weight_hh\"]\n",
    "bias_hh = lstm_cell.state_dict()[\"bias_hh\"]\n",
    "\n",
    "print(\"shape:\")\n",
    "print(f\"W_ih: {W_ih.shape}\\tbias_ih: {bias_ih.shape}\")\n",
    "print(f\"W_hh: {W_hh.shape}\\tbias_hh: {bias_hh.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of weights and biases of the input gate:\n",
      "W_ii: torch.Size([2, 3])\tbias_ii: torch.Size([2])\n",
      "W_hi: torch.Size([2, 2])\tbias_hi: torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "W_ii, W_if, W_ig, W_io = torch.split(W_ih, k)\n",
    "bias_ii, bias_if, bias_ig, bias_io = torch.split(bias_ih, k)\n",
    "W_hi, W_hf, W_hg, W_ho = torch.split(W_hh, k)\n",
    "bias_hi, bias_hf, bias_hg, bias_ho = torch.split(bias_hh, k)\n",
    "\n",
    "print(\"shapes of weights and biases of the input gate:\")\n",
    "print(f\"W_ii: {W_ii.shape}\\tbias_ii: {bias_ii.shape}\")\n",
    "print(f\"W_hi: {W_hi.shape}\\tbias_hi: {bias_hi.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram illustrates the structure of a single LSTM cell:\n",
    "\n",
    "<img src=\"../figures/lstm.png\" width=\"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four internal activation vectors inside an LSTM cell, $i_t$, $f_t$, $g_t$ and $o_t$, are calculated using the following equations:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    i_t &= \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "    f_t &= \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "    g_t &= \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "    o_t &= \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: tensor([0.5944, 0.2809])\n",
      "f: tensor([0.4461, 0.7488])\n",
      "g: tensor([0.2927, 0.4243])\n",
      "o: tensor([0.7073, 0.5732])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(d)\n",
    "h0 = torch.randn(k)\n",
    "c0 = torch.randn(k)\n",
    "\n",
    "# Inpute gate's activation vector\n",
    "i = sigmoid(\n",
    "    W_ii @ x + bias_ii + W_hi @ h0 + bias_hi\n",
    ")\n",
    "\n",
    "# Forget gate's activation vector\n",
    "f = sigmoid(\n",
    "    W_if @ x + bias_if + W_hf @ h0 + bias_hf\n",
    ")\n",
    "\n",
    "# Cell input activation vector\n",
    "g = tanh(\n",
    "    W_ig @ x + bias_ig + W_hg @ h0 + bias_hg\n",
    ")\n",
    "\n",
    "# Ouput gate's activation vector\n",
    "o = sigmoid(\n",
    "    W_io @ x + bias_io + W_ho @ h0 + bias_ho\n",
    ")\n",
    "\n",
    "print(f\"i: {i}\")\n",
    "print(f\"f: {f}\")\n",
    "print(f\"g: {g}\")\n",
    "print(f\"o: {o}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell state vector $c_t$ and hidden state vector $h_t$ are outputted from each LSTM cell:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "    h_t &= o_t \\odot \\tanh(c_t) \n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual calculations of h and c:\n",
      "h: tensor([ 0.3590, -0.3243])\n",
      "c: tensor([ 0.5594, -0.6413])\n"
     ]
    }
   ],
   "source": [
    "# Cell state vector\n",
    "c = f * c0 + i * g\n",
    "\n",
    "# Hidden state vector, i.e.,\n",
    "# output vector of the LSTM cell\n",
    "h = o * tanh(c)\n",
    "\n",
    "print(f\"manual calculations of h and c:\")\n",
    "print(f\"h: {h}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output from the LSTM cell:\n",
      "h: tensor([ 0.3590, -0.3243])\n",
      "c: tensor([ 0.5594, -0.6413])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    h, c = lstm_cell(x, (h0, c0))\n",
    "    \n",
    "print(f\"output from the LSTM cell:\")\n",
    "print(f\"h: {h}\")\n",
    "print(f\"c: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have verified, the manual calculations agree with PyTorch's implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(3, 2)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm = nn.LSTM(\n",
    "    input_size=d,\n",
    "    hidden_size=k,\n",
    ")\n",
    "\n",
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state dict of an LSTM instance is different from that of an LSTM cell for LSTM may have multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0'])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suffix `_l0` is the layer name. If we initialize an LSTM with multiple layers, then there may be `l1`, `l2`, ... \n",
    "\n",
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['weight_ih_l0', 'weight_hh_l0', 'bias_ih_l0', 'bias_hh_l0', 'weight_ih_l1', 'weight_hh_l1', 'bias_ih_l1', 'bias_hh_l1', 'weight_ih_l2', 'weight_hh_l2', 'bias_ih_l2', 'bias_hh_l2'])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LSTM(\n",
    "    input_size=d,\n",
    "    hidden_size=k,\n",
    "    num_layers=3\n",
    ").state_dict().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `lstm` instance has only one layer, it is essentially the same as an LSTM cell. And we can make `lstm` be the same as `lstm_cell` created previously by sharing its state dict. Of course, we must change the keys of the ordered dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = OrderedDict()\n",
    "for key in lstm_cell.state_dict():\n",
    "    new_key = f\"{key}_l0\"\n",
    "    state_dict[new_key] = lstm_cell.state_dict()[key]\n",
    "\n",
    "lstm.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a sequence of input vectors $x_0, x_1, \\ldots, x_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence as a tensor:\n",
      "tensor([[-0.8887,  1.2135,  0.7924],\n",
      "        [-0.4401,  0.4996, -0.7581],\n",
      "        [ 0.9989, -0.8793,  0.7486],\n",
      "        [-1.3375,  0.6449,  0.9652],\n",
      "        [ 1.0090, -0.0337, -1.0090]])\n",
      "\n",
      "sequence as a tuple of tensors:\n",
      "(tensor([-0.8887,  1.2135,  0.7924]),\n",
      " tensor([-0.4401,  0.4996, -0.7581]),\n",
      " tensor([ 0.9989, -0.8793,  0.7486]),\n",
      " tensor([-1.3375,  0.6449,  0.9652]),\n",
      " tensor([ 1.0090, -0.0337, -1.0090]))\n"
     ]
    }
   ],
   "source": [
    "seq_len = 5\n",
    "x_seq = torch.randn(seq_len, d)\n",
    "xs = tuple(map(lambda x: x.squeeze(), x_seq.split(1)))\n",
    "\n",
    "print(\"sequence as a tensor:\")\n",
    "print(x_seq)\n",
    "print()\n",
    "print(\"sequence as a tuple of tensors:\")\n",
    "pprint(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can feed these vectors one after another to the LSTM cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0362, -0.0598])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    h, c = h0, c0\n",
    "    for x in xs:\n",
    "        h, c = lstm_cell(x, (h, c))\n",
    "    out = h\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, equivalently we may pass the entire sequence (as a tensor) to the LSTM module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4519, -0.4052],\n",
       "        [ 0.0638, -0.2567],\n",
       "        [ 0.1473, -0.2891],\n",
       "        [ 0.0493, -0.1647],\n",
       "        [ 0.0362, -0.0598]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out, (ht, ct) = lstm(x_seq, (h0.reshape(1, -1), c0.reshape(1, -1)))\n",
    "\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output tensor `out` consists of all output tensors including the intermediate ones. Note also that we have reshaped the initial hidden and cell state vectors before passing to LSTM. Again, this is because LSTM may have multiple layers. The first of `reshape` is set `1` because there is only one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the last output vector, as are usually of interest, we simply access via last index `out[-1]`. Alternatively, we may use `ht` since they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last output vector from LSTM:\n",
      "tensor([ 0.0362, -0.0598])\n",
      "\n",
      "last hidden state vector:\n",
      "tensor([[ 0.0362, -0.0598]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"last output vector from LSTM:\\n{out[-1]}\")\n",
    "print()\n",
    "print(f\"last hidden state vector:\\n{ht}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air Quality Prediction Using LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original CSV file content\n",
    "with open(\"../data/air-quality/AirQualityUCI.csv\", \"r\") as f:\n",
    "    air_quality_file_content = f.read()\n",
    "\n",
    "# Clean the file content\n",
    "air_quality_file_content = air_quality_file_content.replace(\",\", \".\").replace(\";\", \",\")\n",
    "air_quality_str_io = StringIO(air_quality_file_content)\n",
    "air_quality = pd.read_csv(air_quality_str_io)\n",
    "\n",
    "# Drop empty columns, and then\n",
    "# drop empty rows\n",
    "air_quality = air_quality.dropna(axis=1, how=\"all\").dropna(axis=0, how=\"all\")\n",
    "\n",
    "air_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_to_revised_column_name = {\n",
    "    \"CO(GT)\": \"co\",\n",
    "    \"NO2(GT)\": \"no2\",\n",
    "    \"T\": \"temperature\",\n",
    "    \"RH\": \"relative_humidity\",\n",
    "    \"AH\": \"absolute_humidity\"\n",
    "}\n",
    "\n",
    "air_quality: pd.DataFrame = air_quality.loc[:, original_to_revised_column_name.keys()]\n",
    "air_quality.rename(columns=original_to_revised_column_name, inplace=True)\n",
    "\n",
    "air_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlidingWindowSplitter(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        lookback: int = 1,\n",
    "        forward_horizon: int = 1\n",
    "    ) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lookback = lookback\n",
    "        self.forward_horizon = forward_horizon\n",
    "    \n",
    "    def fit(\n",
    "        self,\n",
    "        seq: np.ndarray,\n",
    "        target_seq: Optional[np.ndarray] = None,\n",
    "    ) -> Self:\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(\n",
    "        self,\n",
    "        seq: np.ndarray,\n",
    "        target_seq: Optional[np.ndarray] = None,\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        \n",
    "        total_seq_length = seq.shape[0]\n",
    "        if target_seq is not None:\n",
    "            assert target_seq.shape[0] == total_seq_length\n",
    "            \n",
    "        rows = []\n",
    "        target_rows = []\n",
    "        window_size = self.lookback + self.forward_horizon\n",
    "        for i in range(total_seq_length - (window_size - 1)):\n",
    "            rows.append(seq[i:i+window_size].copy())\n",
    "            if target_seq is not None:\n",
    "                target_rows.append(target_seq[i:i+window_size].copy())\n",
    "        \n",
    "        # Convert to an array\n",
    "        rows = np.stack(rows)\n",
    "        if target_seq is not None:\n",
    "            target_rows = np.stack(target_rows)\n",
    "        \n",
    "        if target_seq is None:\n",
    "            X = rows[:, :self.lookback]\n",
    "            y = rows[:, self.lookback:]\n",
    "            \n",
    "        else:\n",
    "            X = rows[:, :self.lookback]\n",
    "            y = target_rows[:, self.lookback:]\n",
    "        \n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window_splitter = SlidingWindowSplitter(lookback=3, forward_horizon=1)\n",
    "\n",
    "X, y = sliding_window_splitter.fit_transform(\n",
    "    air_quality.to_numpy(), \n",
    "    target_seq=air_quality.loc[:, [\"co\", \"no2\"]].to_numpy(),\n",
    ")\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linguaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
